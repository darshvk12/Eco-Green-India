# -*- coding: utf-8 -*-
"""rag-gamify.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1YkBPvkYp7oSS1YMHOJa8zn1b-nIp_bVr
"""

!pip install langchain langchain-google-genai langchain_community pypdf chromadb sentence-transformers -q
!pip install google-generativeai pdfplumber -q

import os
import pdfplumber
import google.generativeai as genai
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_google_genai import GoogleGenerativeAIEmbeddings, ChatGoogleGenerativeAI
from langchain_community.vectorstores import Chroma

os.environ["GOOGLE_API_KEY"] = "AIzaSyB7-vj_AQgr-q61QAZX45u_zsJkTgW-Vt4"

def parse_pdfs(pdf_paths):
    """
    Extract text from multiple PDFs.
    """
    if isinstance(pdf_paths, str):
        pdf_paths = [pdf_paths]

    all_text = ""
    for path in pdf_paths:
        try:
            with pdfplumber.open(path) as pdf:
                for page in pdf.pages:
                    all_text += page.extract_text() or ""
            print(f"‚úÖ Extracted text from {path}")
        except Exception as e:
            print(f"‚ö†Ô∏è Failed to read {path}: {e}")

    return all_text if all_text.strip() else None

def create_document_chunks(text):
    """
    Split extracted PDF text into smaller chunks.
    """
    if not text or not isinstance(text, str):
        raise ValueError("Input text is empty or invalid")

    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=500,
        chunk_overlap=100,
        length_function=len,
        separators=["\n\n", "\n", " ", ""]
    )

    chunks = text_splitter.split_text(text)
    print(f"‚úÖ Document split into {len(chunks)} chunks")
    return chunks

!pip install faiss-cpu -q

from langchain_community.vectorstores import FAISS

# ================== STORE EMBEDDINGS ==================
def store_embeddings(text_chunks):
    """
    Store document chunks in FAISS with embeddings.
    """
    embedding_model = GoogleGenerativeAIEmbeddings(
        model="models/text-embedding-004"
    )

    faiss_store = FAISS.from_texts(
        texts=text_chunks,
        embedding=embedding_model
    )

    print(f"‚úÖ Stored {len(text_chunks)} chunks in FAISS")
    return faiss_store, embedding_model

# ================== RETRIEVAL ==================
def retrieve_chunks(vectorstore, query, k=3):
    retriever = vectorstore.as_retriever(
        search_type="similarity",
        search_kwargs={"k": k}
    )
    return retriever.get_relevant_documents(query)

def get_context_from_chunks(relevant_chunks, splitter="\n\n---\n\n"):
    chunk_contents = []
    for i, chunk in enumerate(relevant_chunks):
        if hasattr(chunk, "page_content"):
            chunk_contents.append(f"[Chunk {i+1}]: {chunk.page_content}")
    return splitter.join(chunk_contents)

def generate_response(prompt, model="gemini-2.0-flash", temperature=0.2):
    llm = ChatGoogleGenerativeAI(
        model=model,
        temperature=temperature,
        top_p=0.95
    )
    response = llm.invoke(prompt)
    return response.content

pdf_paths = [
    "/content/Climate.pdf",
    "/content/Pollution.pdf",
    "/content/waste.pdf",
    "/content/tree.pdf"
]

# 1. Parse PDFs
text_data = parse_pdfs(pdf_paths)

# 2. Chunking
text_chunks = create_document_chunks(text_data)

# 3. Store embeddings
faiss_store, embedding_model = store_embeddings(text_chunks)

while True:
    user_query = input("Ask your question (or type 'exit' to quit): ")
    if user_query.lower() == "exit":
        print("üëã Goodbye!")
        break

    # Retrieve chunks
    relevant_chunks = retriever.invoke(user_query)
    context = get_context_from_chunks(relevant_chunks)

    # Build final prompt
    final_prompt = f"""
    You are a helpful assistant answering questions based on provided context.

    Context:
    {context}

    Question:
    {user_query}

    Answer:
    """

    # Generate response
    answer = generate_response(final_prompt)
    print(f"\nü§ñ Answer: {answer}\n")